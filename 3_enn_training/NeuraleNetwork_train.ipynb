{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras import regularizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import LearningRateScheduler,EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras.losses import MeanSquaredError\n",
    "from tensorflow.keras import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SurfaceData=pd.read_csv('SurfaceData_SUM.csv',delimiter=\";\") # load training database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(SurfaceData)):\n",
    "    SurfaceData[\"bins\"][i]=np.fromstring(SurfaceData[\"bins\"][i][1:-2],sep=',')\n",
    "    SurfaceData[\"n0\"][i]=np.fromstring(SurfaceData[\"n0\"][i][1:-2],sep=',')\n",
    "    SurfaceData[\"Qquerys\"][i]=np.fromstring(SurfaceData[\"Qquerys\"][i][1:-2],sep=',')\n",
    "    SurfaceData[\"PSquerys\"][i]=np.fromstring(SurfaceData[\"PSquerys\"][i][1:-2],sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SurfaceData_Simulated=SurfaceData[SurfaceData[\"DU\"].notna()]# Exclude not simulated samples\n",
    "SurfaceData_Simulated=SurfaceData_Simulated[SurfaceData[\"DU\"]>=6] # Exclude transitionally rough samples\n",
    "SurfaceData_Pool=SurfaceData[SurfaceData[\"DU\"].isna()]# Put not simulated samples to repository\n",
    "ListOfLabledID=SurfaceData_Simulated[\"Surface_ID\"].to_numpy()\n",
    "krTrain=SurfaceData_Simulated[\"kr\"].to_numpy()\n",
    "SurfacePoolID=SurfaceData_Pool[\"Surface_ID\"].to_numpy()\n",
    "\n",
    "Iteration_num=50 # number of NN members\n",
    "SurfaceTrain=[]\n",
    "SurfacePool=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in ListOfLabledID.astype(int): #Formulate input vectors of trainning data\n",
    "    SurfaceTrain_row=[SurfaceData.iloc[i].kt/SurfaceData.iloc[i].K_99,SurfaceData.iloc[i].lambda0_k99,SurfaceData.iloc[i].lambda1_k99]\n",
    "    SurfaceTrain_row.extend(SurfaceData.iloc[i].n0)\n",
    "    SurfaceTrain_row.extend(SurfaceData.iloc[i].PSquerys)\n",
    "    SurfaceTrain.append(SurfaceTrain_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in SurfacePoolID.astype(int): #Formulate input vectors of rest data in repository \n",
    "    SurfacePool_row=[SurfaceData.iloc[i].kt/SurfaceData.iloc[i].K_99,SurfaceData.iloc[i].lambda0_k99,SurfaceData.iloc[i].lambda1_k99]\n",
    "    SurfacePool_row.extend(SurfaceData.iloc[i].n0)\n",
    "    SurfacePool_row.extend(SurfaceData.iloc[i].PSquerys)\n",
    "    SurfacePool.append(SurfacePool_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SurfaceTrain=np.array(SurfaceTrain)\n",
    "SurfacePool=np.array(SurfacePool)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_scheduler(epoch, learning_rate):\n",
    "    #lr = learning_rate\n",
    "    if epoch == 1000:\n",
    "        learning_rate = learning_rate*0.5\n",
    "        return learning_rate\n",
    "    elif epoch == 1500:\n",
    "        learning_rate = learning_rate*0.5\n",
    "        return learning_rate\n",
    "    elif epoch == 1800:\n",
    "        learning_rate = learning_rate*0.5\n",
    "        return learning_rate\n",
    "    else:\n",
    "        #learning_rate = lr\n",
    "        return learning_rate\n",
    "    \n",
    "new_lr = LearningRateScheduler(lr_scheduler, verbose=0)\n",
    "def build_model(space):\n",
    "    model = Sequential()\n",
    "    input_shape = 63\n",
    "    if space[3]=='leakyrelu':\n",
    "        model.add(Dense(int(space[0]),input_shape=(input_shape,),activation=LeakyReLU(alpha=0.1),kernel_regularizer=regularizers.l2(space[5])))\n",
    "    else:\n",
    "        model.add(Dense(int(space[0]),input_shape=(input_shape,),activation=Activation(space[3]),kernel_regularizer=regularizers.l2(space[5])))\n",
    "    for i in range(2):\n",
    "        model.add(Dense(int(space[i+1]),kernel_regularizer=regularizers.l2(space[5])))\n",
    "        if space[3] == 'leakyrelu':\n",
    "            model.add(LeakyReLU(alpha=0.1))\n",
    "        else:\n",
    "            model.add(Activation(space[3]))\n",
    "    # Add output layer\n",
    "    model.add(Dense(1,kernel_regularizer=regularizers.l2(space[5])))\n",
    "    if space[3] == 'leakyrelu':\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "    else:\n",
    "        model.add(Activation(space[3]))\n",
    "    # Compile the model\n",
    "    model.compile(optimizer=Adam(space[4]), loss=MeanSquaredError(),\n",
    "                  metrics=[metrics.MeanSquaredError(),metrics.MeanAbsolutePercentageError(name=\"MAPE\")])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions=[]\n",
    "losshistory=[]\n",
    "vallosshistory=[]\n",
    "best_params_csv=pd.read_csv('./Hyperparameters.csv',sep=\";\") # read hyperparameters\n",
    "for i in range(Iteration_num): # iteratively train NN members\n",
    "    p = np.random.permutation(len(SurfaceTrain)) # Schuffle training data\n",
    "    SurfaceTrain=SurfaceTrain[p]\n",
    "    krTrain=krTrain[p]\n",
    "    NN=build_model(best_params_csv.iloc[0].values.tolist())\n",
    "    NN.summary()\n",
    "    es = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=2000,restore_best_weights=True)# Set early stopping, if necessary\n",
    "    callbacks_list = [es,new_lr]\n",
    "    history=NN.fit(SurfaceTrain,np.array(krTrain),epochs=2500,validation_split=0.1,callbacks=callbacks_list)\n",
    "    losshistory.append(history.history['loss'])\n",
    "    vallosshistory.append(history.history['val_loss'])\n",
    "    predictions.append(NN.predict(SurfacePool))\n",
    "    NN.save('./Models/model'+str(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_params_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "N_samples=np.shape(predictions)[1]\n",
    "predictions=np.array(predictions)\n",
    "Uncertainty=[np.std(predictions[:,j]) for j in range(N_samples)]# Model uncertainty is the standard deviation of the predictions\n",
    "Prediction=[np.mean(predictions[:,j]) for j in range(N_samples)]# Model prediction is the averaged prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "predict_pool=[]\n",
    "## An instance of model prediction, note that the current testing datasets are the training datasets\n",
    "for i in range(Iteration_num):\n",
    "    Model=load_model(\"./Models/model\"+str(i))\n",
    "    predict_pool.append(Model.predict(SurfacePool))\n",
    "predict_pool=np.array(predict_train)\n",
    "\n",
    "predict_uncertainty=[]\n",
    "prediction=[]\n",
    "for i in range(len(SurfacePool)):\n",
    "    predict_uncertainty.append(np.std(predict_pool[:,i]))\n",
    "    prediction.append(np.mean(predict_train[:,i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_input={'PoolID':SurfacePoolID.astype(int),'Prediction':Prediction,'Uncertainty':Uncertainty,'Prediction_Distribution':[predictions[:,i] for i in range(len(SurfacePoolID))]}\n",
    "SurfacePoolData=pd.DataFrame(data=data_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check uncertainties of roughenss predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SurfacePoolData=SurfacePoolData.set_index('PoolID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nominate next interested samples following AL framwork\n",
    "AL_next_ID=list(SurfacePoolData.sort_values(\"Uncertainty\",ascending=False).head(20).index)\n",
    "AL_next_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Uncertainty_copy=Uncertainty.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "Uncertainty_copy.sort(reverse=True)\n",
    "plt.plot(Uncertainty_copy,label='Prediction variance')\n",
    "#plt.plot(Uncertainty[0:20],label='LC surfaces')\n",
    "plt.legend()\n",
    "plt.ylabel('Prediction variance')\n",
    "plt.xlabel('Surface,sorted by uncertainty')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
